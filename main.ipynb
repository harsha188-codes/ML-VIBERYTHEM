{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YgvxttnkWtF3",
    "outputId": "74b776a1-95ca-4482-b4fa-cbf9bea1569c"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = 'https://www.dropbox.com/s/nilt43hyl1dx82k/dataset.zip?dl=1'  # Use dl=1 to directly download without viewing in the browser\n",
    "destination = 'dataset.zip'\n",
    "\n",
    "urllib.request.urlretrieve(url, destination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URoUsT18cYq5"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WcbKQXMVcbBU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rx9TgAO9W00x"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('dataset.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('')  # Extract the contents to the current directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-IGz-CkRW219"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator , img_to_array, load_img\n",
    "from keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "from keras.losses import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G7IFGcrZEqbv"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import cv2\n",
    "from keras.models import model_from_json\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import cv2\n",
    "from keras.models import model_from_json\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIyjAC_jErkA"
   },
   "outputs": [],
   "source": [
    "train_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "validation_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "#ImageDataGenerator- preprocessing module for generating batches of augmented or preprocessed image data.\n",
    "\n",
    "# creates an instance of the ImageDataGenerator class called train_data_gen for generating batches of training data. \n",
    "#The rescale parameter is set to 1./255, which means that each pixel value in the images will be rescaled to the range [0, 1]. \n",
    "#This rescaling is a common preprocessing step in deep learning to normalize the pixel values, as it helps improve convergence during training and can lead to better performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZdoPXkJHE2rO",
    "outputId": "ea3fbdab-3c4a-4a43-dfab-465634e33bff"
   },
   "outputs": [],
   "source": [
    "train_generator = train_data_gen.flow_from_directory(\n",
    "\t'train',\n",
    "\ttarget_size=(48, 48),    #specifies the dimensions to which the input images will be resized\n",
    "\tbatch_size=64,           #size of the batches of images that will be generated during training. In this case, each batch will contain 64 images.\n",
    "\tcolor_mode=\"grayscale\",\n",
    "\tclass_mode='categorical') #specifies the type of labels that will be generated for the images. Setting it to 'categorical' \n",
    "                               #indicates that the labels will be represented as one-hot encoded vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gJuROmA8E4jG",
    "outputId": "8dc5c06d-a376-46fe-8568-83b6a1b139db"
   },
   "outputs": [],
   "source": [
    "validation_generator = validation_data_gen.flow_from_directory(\n",
    "\t\t'test',\n",
    "\t\ttarget_size=(48, 48),\n",
    "\t\tbatch_size=64,\n",
    "\t\tcolor_mode=\"grayscale\",\n",
    "\t\tclass_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0r-5uYTuE6h2",
    "outputId": "f0abf9ef-a8a7-4187-8cac-997cebd6f34e"
   },
   "outputs": [],
   "source": [
    "emotion_model = Sequential()       #initializes a sequential model, allowing layers to be added sequentially.\n",
    "\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu',\n",
    "\t\t\t\t\t\tinput_shape=(48, 48, 1)))                   #adds a 2D convolutional layer with 32 filters, each with a 3x3 kernel size, ReLU activation function,\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))                                    #This adds dropout regularization with a rate of 25% to prevent overfitting.\n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Flatten())           #flattens the output of the previous layer into a one-dimensional array.\n",
    "emotion_model.add(Dense(1024, activation='relu')) #adds a fully connected layer with 1024 neurons and ReLU activation.\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))  #output layer\n",
    "\n",
    "emotion_model.summary()  #prints a summary of the model architecture, including the type of each layer, the output shape, and the number of trainable parameters.\n",
    "\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "initial_learning_rate = 0.0001\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps=100000,\n",
    "\t\t\t\t\t\t\tdecay_rate=0.96) #creates a learning rate schedule using exponential decay, which decreases the learning rate over time.\n",
    "\n",
    "optimizer = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "emotion_model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n",
    "\t\t\t\t\tmetrics=['accuracy'])\n",
    "#loss function as categorical cross-entropy (suitable for multi-class classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71iP1ArSE9QO",
    "outputId": "6c2bf8d5-bcc6-4255-b768-64cc5b3e68b1"
   },
   "outputs": [],
   "source": [
    "emotion_model_info = emotion_model.fit(\n",
    "\t\ttrain_generator,\n",
    "\t\tsteps_per_epoch=28709 // 64,\n",
    "\t\tepochs=30,\n",
    "\t\tvalidation_data=validation_generator,\n",
    "\t\tvalidation_steps=7178 // 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M6jB0e2AFBq2",
    "outputId": "2763b79d-1262-42a9-b87f-e9f78d5a57f4"
   },
   "outputs": [],
   "source": [
    "emotion_model.evaluate(validation_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hqQLD__LFE8H"
   },
   "outputs": [],
   "source": [
    "accuracy = emotion_model_info.history['accuracy']\n",
    "val_accuracy = emotion_model_info.history['val_accuracy']\n",
    "loss = emotion_model_info.history['loss']\n",
    "val_loss = emotion_model_info.history['val_loss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "XWS_n0cuFH2T",
    "outputId": "1ef78a81-b6d0-4c15-eb67-276cd6796c7b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Accuracy graph\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(accuracy, label='accuracy')\n",
    "plt.plot(val_accuracy, label='val accuracy')\n",
    "plt.title('Accuracy Graph')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# loss graph\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss, label='loss')\n",
    "plt.plot(val_loss, label='val loss')\n",
    "plt.title('Loss Graph')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYskHa9RFKqi"
   },
   "outputs": [],
   "source": [
    "\n",
    "model_json = emotion_model.to_json()\n",
    "with open(\"emotion_model.json\", \"w\") as json_file:\n",
    "\tjson_file.write(model_json)\n",
    "\n",
    "# save trained model weight in .h5 file\n",
    "emotion_model.save_weights('emotion_model.weights.h5')\n",
    "\n",
    "\n",
    "#The HDF5 file format is commonly used for storing large numerical datasets and is suitable for saving the parameters (weights) of deep learning models.\n",
    "#The weights are saved separately from the model architecture because they may be reused or transferred to another model with a compatible architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jUlFFq9bFO0U"
   },
   "outputs": [],
   "source": [
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\",\n",
    "\t\t\t\t3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akPC0KUrFQ7z"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "json_file = open('emotion_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "emotion_model = model_from_json(loaded_model_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yns7y_Ms368Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "8zsRyk1c1QYB",
    "outputId": "ec2aa631-d353-4222-ad8c-6ead6706da27"
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python==4.7.0.68\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "GzFN9OjMFTRA",
    "outputId": "1a44a05e-94c8-438e-a934-e2880d377b21"
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# cap=cv2.VideoCapture(0,cv2.CAP_DSHOW)\n",
    "# while True:\n",
    "#     # Find haar cascade to draw bounding box around face\n",
    "#     ret, frame = cap.read()\n",
    "#     print(\"Retrieved frame:\", ret)  # Add this line to check the value of ret\n",
    "#     frame = cv2.resize(frame, (1280, 720))\n",
    "#     if not ret:\n",
    "#         print(ret)\n",
    "#         continue\n",
    "\n",
    "#     # Create a face detector\n",
    "#     face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "#     gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#     # detect faces available on camera\n",
    "#     num_faces = face_detector.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "#     # take each face available on the camera and Preprocess it\n",
    "#     for (x, y, w, h) in num_faces:\n",
    "#         cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (0, 255, 0), 4)\n",
    "#         roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "#         cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "\n",
    "#         # predict the emotions\n",
    "#         emotion_prediction = emotion_model.predict(cropped_img)\n",
    "#         maxindex = int(np.argmax(emotion_prediction))\n",
    "#         cv2.putText(frame, emotion_dict[maxindex], (x+5, y-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "#     cv2.imshow('Emotion Detection', frame)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# from keras.models import model_from_json\n",
    "\n",
    "# # Load the pre-trained facial emotion recognition model from the JSON file\n",
    "# with open('emotion_model.json', 'r') as json_file:\n",
    "#     loaded_model_json = json_file.read()\n",
    "# emotion_model = model_from_json(loaded_model_json)\n",
    "# # Load weights into the model\n",
    "# emotion_model.load_weights(\"emotion_model.h5\")  # Assuming weights are stored in emotion_model.h5\n",
    "\n",
    "# # Define emotion labels\n",
    "# emotion_dict = {0: \"Angry\", 1: \"Disgust\", 2: \"Fear\", 3: \"Happy\", 4: \"Sad\", 5: \"Surprise\", 6: \"Neutral\"}\n",
    "\n",
    "\n",
    "\n",
    "# cap=cv2.VideoCapture(0,cv2.CAP_DSHOW)\n",
    "# while True:\n",
    "#     # Find haar cascade to draw bounding box around face\n",
    "#     ret, frame = cap.read()\n",
    "#     print(\"Retrieved frame:\", ret)  # Add this line to check the value of ret\n",
    "#     # frame = cv2.resize(frame, (1280, 720))\n",
    "#     frame = cv2.resize(frame, (500,300))\n",
    "#     if not ret:\n",
    "#         print(ret)\n",
    "#         continue\n",
    "\n",
    "#     # Create a face detector\n",
    "#     face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "#     gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#     # detect faces available on camera\n",
    "#     num_faces = face_detector.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "#     # take each face available on the camera and Preprocess it\n",
    "#     for (x, y, w, h) in num_faces:\n",
    "#         cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (0, 255, 0), 4)\n",
    "#         roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "#         cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "\n",
    "#         # predict the emotions\n",
    "#         emotion_prediction = emotion_model.predict(cropped_img)\n",
    "#         maxindex = int(np.argmax(emotion_prediction))\n",
    "#         cv2.putText(frame, emotion_dict[maxindex], (x+5, y-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "#     cv2.imshow('Emotion Detection', frame)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# from keras.models import model_from_json\n",
    "\n",
    "# # Load the pre-trained facial emotion recognition model from the JSON file\n",
    "# with open('emotion_model.json', 'r') as json_file:\n",
    "#     loaded_model_json = json_file.read()\n",
    "# emotion_model = model_from_json(loaded_model_json)\n",
    "# # Load weights into the model\n",
    "# emotion_model.load_weights(\"emotion_model.h5\")  # Assuming weights are stored in emotion_model.h5\n",
    "\n",
    "# # Define emotion labels\n",
    "# emotion_dict = {0: \"Angry\", 1: \"Disgust\", 2: \"Fear\", 3: \"Happy\", 4: \"Sad\", 5: \"Surprise\", 6: \"Neutral\"}\n",
    "\n",
    "# # Create a face detector\n",
    "# face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# # Initialize the video capture object\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# # Check if the camera is opened successfully\n",
    "# if not cap.isOpened():\n",
    "#     print(\"Error: Unable to open camera.\")\n",
    "#     exit()\n",
    "\n",
    "# # Loop for capturing and processing frames\n",
    "# while True:\n",
    "#     # Read a frame from the camera\n",
    "#     ret, frame = cap.read()\n",
    "    \n",
    "#     # Check if the frame is retrieved successfully\n",
    "#     if not ret:\n",
    "#         print(\"Error: Unable to retrieve frame.\")\n",
    "#         break\n",
    "    \n",
    "#     # Resize the frame\n",
    "#     frame = cv2.resize(frame, (500, 300))\n",
    "    \n",
    "#     # Convert the frame to grayscale\n",
    "#     gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#     # Detect faces in the frame\n",
    "#     num_faces = face_detector.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "#     # Process each detected face\n",
    "#     for (x, y, w, h) in num_faces:\n",
    "#         cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (0, 255, 0), 4)\n",
    "#         roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "#         cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "\n",
    "#         # Predict emotions\n",
    "#         emotion_prediction = emotion_model.predict(cropped_img)\n",
    "#         maxindex = int(np.argmax(emotion_prediction))\n",
    "#         cv2.putText(frame, emotion_dict[maxindex], (x+5, y-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "#     # Display the processed frame\n",
    "#     cv2.imshow('Emotion Detection', frame)\n",
    "    \n",
    "#     # Check for key press to exit\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # Release the video capture object and close all windows\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# from keras.models import model_from_json\n",
    "# import time\n",
    "\n",
    "# # Load the pre-trained facial emotion recognition model from the JSON file\n",
    "# with open('emotion_model.json', 'r') as json_file:\n",
    "#     loaded_model_json = json_file.read()\n",
    "# emotion_model = model_from_json(loaded_model_json)\n",
    "# # Load weights into the model\n",
    "# emotion_model.load_weights(\"emotion_model.h5\")  # Assuming weights are stored in emotion_model.h5\n",
    "\n",
    "# # Define emotion labels\n",
    "# emotion_dict = {0: \"Angry\", 1: \"Disgust\", 2: \"Fear\", 3: \"Happy\", 4: \"Sad\", 5: \"Surprise\", 6: \"Neutral\"}\n",
    "\n",
    "# # Create a face detector\n",
    "# face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# # Initialize the video capture object\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# # Check if the camera is opened successfully\n",
    "# if not cap.isOpened():\n",
    "#     print(\"Error: Unable to open camera.\")\n",
    "#     exit()\n",
    "\n",
    "# # Loop for capturing and processing frames\n",
    "# start_time = time.time()\n",
    "# emotion_captured = False\n",
    "# while time.time() - start_time < 5:\n",
    "#     # Read a frame from the camera\n",
    "#     ret, frame = cap.read()\n",
    "    \n",
    "#     # Check if the frame is retrieved successfully\n",
    "#     if not ret:\n",
    "#         print(\"Error: Unable to retrieve frame.\")\n",
    "#         break\n",
    "    \n",
    "#     # Resize the frame\n",
    "#     frame = cv2.resize(frame, (500, 300))\n",
    "    \n",
    "#     # Convert the frame to grayscale\n",
    "#     gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#     # Detect faces in the frame\n",
    "#     num_faces = face_detector.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "#     # Process each detected face\n",
    "#     for (x, y, w, h) in num_faces:\n",
    "#         cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (0, 255, 0), 4)\n",
    "#         roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "#         cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "\n",
    "#         # Predict emotions\n",
    "#         emotion_prediction = emotion_model.predict(cropped_img)\n",
    "#         maxindex = int(np.argmax(emotion_prediction))\n",
    "#         cv2.putText(frame, emotion_dict[maxindex], (x+5, y-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "#         # Print emotion in the terminal\n",
    "#         if not emotion_captured:\n",
    "#             print(\"Emotion detected:\", emotion_dict[maxindex])\n",
    "#             emotion_captured = True\n",
    "\n",
    "#     # Display the processed frame\n",
    "#     cv2.imshow('Emotion Detection', frame)\n",
    "    \n",
    "#     # Check for key press to exit\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # Release the video capture object and close all windows\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "import time\n",
    "\n",
    "# Load the pre-trained facial emotion recognition model from the JSON file\n",
    "with open('emotion_model.json', 'r') as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "emotion_model = model_from_json(loaded_model_json)\n",
    "# Load weights into the model\n",
    "emotion_model.load_weights(\"emotion_model.weights.h5\")  # Assuming weights are stored in emotion_model.h5\n",
    "\n",
    "# Define emotion labels\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Sad\", 5: \"Surprise\", 6: \"Neutral\"}\n",
    "\n",
    "# Create a face detector\n",
    "face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Initialize the video capture object\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the camera is opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open camera.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize variables to track emotions and time\n",
    "start_time = time.time()\n",
    "emotion_count = {label: 0 for label in emotion_dict.values()}\n",
    "\n",
    "# Loop for capturing and processing frames\n",
    "while time.time() - start_time < 8:\n",
    "    # Read a frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Check if the frame is retrieved successfully\n",
    "    if not ret:\n",
    "        print(\"Error: Unable to retrieve frame.\")\n",
    "        break\n",
    "    \n",
    "    # Resize the frame\n",
    "    frame = cv2.resize(frame, (500, 300))\n",
    "    \n",
    "    # Convert the frame to grayscale\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    num_faces = face_detector.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    # Process each detected face\n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (0, 255, 0), 4)\n",
    "\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "\n",
    "        # Predict emotions\n",
    "        emotion_prediction = emotion_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(emotion_prediction))\n",
    "        emotion_label = emotion_dict[maxindex]\n",
    "        \n",
    "        # Update emotion count\n",
    "        emotion_count[emotion_label] += 1\n",
    "\n",
    "    # Display the processed frame\n",
    "    cv2.imshow('Emotion Detection', frame)\n",
    "    \n",
    "    # Check for key press to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Determine the most dominant emotion detected within the 5-second window\n",
    "max_emotion = max(emotion_count, key=emotion_count.get)\n",
    "\n",
    "# Display the most dominant emotion\n",
    "print(\"Most dominant emotion within 5 seconds:\", max_emotion)\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "if max_emotion == \"Fearful\" or max_emotion == \"Disgusted\":\n",
    "    max_emotion = \"Calm\"\n",
    "elif max_emotion == \"Happy\" or max_emotion == \"Sad\" or max_emotion == \"Neutral\":\n",
    "    max_emotion = \"Intense\"\n",
    "elif max_emotion == \"Angry\" or max_emotion == \"Surprised\":\n",
    "    max_emotion = \"Intense\"\n",
    "\n",
    "user_emotion=max_emotion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('small.csv')\n",
    "\n",
    "# Features to be used for similarity calculation\n",
    "features = ['danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']\n",
    "\n",
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes for each song\n",
    "for idx, row in df.iterrows():\n",
    "    G.add_node(row['track_name'], emotion=row['emotion'])\n",
    "\n",
    "# Calculate similarity based on Euclidean distance\n",
    "def calculate_similarity(song1, song2):\n",
    "    # Select features for the two songs\n",
    "    features1 = [song1[feature] for feature in features]\n",
    "    features2 = [song2[feature] for feature in features]\n",
    "\n",
    "    # Calculate Euclidean distance as dissimilarity metric\n",
    "    similarity = cosine_similarity([features1], [features2])[0][0]\n",
    "    return similarity\n",
    "\n",
    "# Add edges between songs based on similarity\n",
    "for i in range(len(df)):\n",
    "    for j in range(i+1, len(df)):\n",
    "        similarity = calculate_similarity(df.iloc[i], df.iloc[j])\n",
    "        if similarity > 0.11:  # Adjust this threshold based on your needs\n",
    "            G.add_edge(df.iloc[i]['track_name'], df.iloc[j]['track_name'], weight=similarity)\n",
    "\n",
    "# Visualization\n",
    "pos = nx.spring_layout(G)  # Positioning nodes using spring layout algorithm\n",
    "nx.draw(G, pos, with_labels=True, font_size=8, node_size=100)\n",
    "edge_labels = nx.get_edge_attributes(G, 'weight')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n",
    "plt.show()\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "# Assuming G is your graph object\n",
    "\n",
    "# Save the graph to a file\n",
    "nx.write_graphml(G, \"graph_data.graphml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(user_emotion)\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "# Read the graph from the saved GraphML file\n",
    "G = nx.read_graphml(\"graph_data.graphml\")\n",
    "\n",
    "def get_similar_songs(user_emotion):\n",
    "    # List to store songs related to user's emotion\n",
    "    related_songs = []\n",
    "\n",
    "    # Find songs related to user's emotion\n",
    "    for node, data in G.nodes(data=True):\n",
    "        if user_emotion.lower() == data[\"emotion\"].lower():\n",
    "            related_songs.append(node)\n",
    "\n",
    "    # If no songs found for the user's emotion\n",
    "    if not related_songs:\n",
    "        return \"No songs found for the given emotion.\"\n",
    "\n",
    "    # List to store similar songs\n",
    "    similar_songs = []\n",
    "\n",
    "    # Find similar songs for each related song\n",
    "    for song in related_songs:\n",
    "        neighbors = list(G.neighbors(song))\n",
    "        for neighbor in neighbors:\n",
    "            if neighbor not in similar_songs and neighbor not in related_songs:\n",
    "                similar_songs.append(neighbor)\n",
    "\n",
    "    recommended_songs = related_songs[:5] + similar_songs[:5]  # Adjust the number as per your preference\n",
    "    return recommended_songs\n",
    "           \n",
    "    \n",
    "    # Return top 10 similar songs\n",
    "    return related_songs[:5]\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "user_emotion = \"happy\"\n",
    "similar_songs = get_similar_songs(user_emotion)\n",
    "print(similar_songs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Assuming G is your graph object\n",
    "G = nx.read_graphml(\"graph_data.graphml\")\n",
    "\n",
    "def get_similar_songs(user_emotion):\n",
    "    # List to store songs related to user's emotion\n",
    "    related_songs = []\n",
    "\n",
    "    # Find songs related to user's detected emotion\n",
    "    for node, data in G.nodes(data=True):\n",
    "        if user_emotion.lower() == data[\"emotion\"].lower():\n",
    "            related_songs.append(node)\n",
    "\n",
    "    # If no songs found for the user's emotion\n",
    "    if not related_songs:\n",
    "        return \"No songs found for the given emotion.\"\n",
    "\n",
    "    # List to store similar songs\n",
    "    similar_songs = []\n",
    "\n",
    "    # Find similar songs for each related song\n",
    "    for song in related_songs:\n",
    "        neighbors = list(G.neighbors(song))\n",
    "        for neighbor in neighbors:\n",
    "            if neighbor not in similar_songs and neighbor not in related_songs:\n",
    "                similar_songs.append(neighbor)\n",
    "\n",
    "    # Combine related and similar songs, adjust number as per your preference\n",
    "    recommended_songs = related_songs[:5] + similar_songs[:5]\n",
    "\n",
    "    # Return the recommended songs\n",
    "    return recommended_songs\n",
    "\n",
    "# Assuming you have detected the emotion and stored it in max_emotion or user_emotion\n",
    "user_emotion = max_emotion\n",
    "\n",
    "# Call the function with the detected emotion\n",
    "similar_songs = get_similar_songs(user_emotion)\n",
    "\n",
    "# Print or use the recommended songs\n",
    "print(\"Recommended Songs for\", user_emotion, \":\", similar_songs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_emotion = \"calm\"\n",
    "similar_songs = get_similar_songs(user_emotion)\n",
    "print(similar_songs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
